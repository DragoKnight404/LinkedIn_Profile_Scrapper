{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "from time import sleep\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "url = \"https://www.linkedin.com/in/laxmimerit\"  # profile to scrape\n",
    "driver = webdriver.Chrome()  # start a new window with chrome web browser\n",
    "wait = WebDriverWait(driver, 10)  # WebDriverWait instance with a 10-second timeout\n",
    "profile_data = {}  # dictionary to store profile data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headline fetched  {'name': 'Laxmi Kant', 'url': 'https://www.linkedin.com/in/laxmimerit', 'headline': 'Gen AI in Finance & Investment Services | Data Scientist | IIT Kharagpur | Asset Management | AI-Driven Financial Modeling | Search Ranking | NLP Python BERT AWS Elasticsearch GNN SQL LLM | AI in Investment Strategies'}\n",
      "ABOUT DONE                  = \n",
      " Demonstrated 8+ years of expertise in advanced analytics as an AVP in Data Science, showcasing dynamic and impactful contributions. Seeking to leverage expertise in customer behavior modeling, personalized marketing, product discovery & search optimization, and recommendations to drive impactful solutions in the fields of Data Science, Machine Learning, and Artificial Intelligence.ðŸ‘‰ Significant Achievements Across Careerâ–ª Strategically led impactful initiatives in Customer Behavior Modeling, resulting in a substantial 30% increase in customer retention and a 10% reduction in advertising spending.â–ª Pioneered the development and implementation of a Machine Learning product relevance ranking system, contributing to a remarkable 30% increase in conversion rates and a 20% growth in revenue.â–ª Applied innovative techniques, including Social Graph Analytics and Predictive  Modeling, achieving a 35% improvement in recommendation conversion rates and a 40% increase in customer care calling conversion rates.â–ª Revolutionized customer segmentation and ranking, realizing a notable 20% improvement in conversion rates.â–ª Engineered predictive models for email open rates, driving a remarkable 70% improvement in email engagement.â–ª Achieved transformative outcomes, including a substantial 50% improvement in search conversion rates, a notable 60% increase in revenue, and an impressive 90% reduction in zero-result queries.â–ª A strategic driver of user satisfaction and engagement, evidenced by a commendable 10% decrease in exit rates.Demonstrated 8+ years of expertise in advanced analytics as an AVP in Data Science, showcasing dynamic and impactful contributions. Seeking to leverage expertise in customer behavior modeling, personalized marketing, product discovery & search optimization, and recommendations to drive impactful solutions in the fields of Data Science, Machine Learning, and Artificial Intelligence.\n",
      "\n",
      "ðŸ‘‰ Significant Achievements Across Career\n",
      "â–ª Strategically led impactful initiatives in Customer Behavior Modeling, resulting in a substantial 30% increase in customer retention and a 10% reduction in advertising spending.\n",
      "\n",
      "â–ª Pioneered the development and implementation of a Machine Learning product relevance ranking system, contributing to a remarkable 30% increase in conversion rates and a 20% growth in revenue.\n",
      "\n",
      "â–ª Applied innovative techniques, including Social Graph Analytics and Predictive  Modeling, achieving a 35% improvement in recommendation conversion rates and a 40% increase in customer care calling conversion rates.\n",
      "\n",
      "â–ª Revolutionized customer segmentation and ranking, realizing a notable 20% improvement in conversion rates.\n",
      "\n",
      "â–ª Engineered predictive models for email open rates, driving a remarkable 70% improvement in email engagement.\n",
      "\n",
      "â–ª Achieved transformative outcomes, including a substantial 50% improvement in search conversion rates, a notable 60% increase in revenue, and an impressive 90% reduction in zero-result queries.\n",
      "\n",
      "â–ª A strategic driver of user satisfaction and engagement, evidenced by a commendable 10% decrease in exit rates.\n",
      "EXPERIENCE DONE                   =\n",
      " [{'company_name': 'IGP', 'duration': 'Full-time Â· 4 yrs 10 mos', 'designations': [{'designation': 'Assistant Vice President', 'duration': 'Oct 2023 to Sep 2024 Â· 1 yr', 'location': 'Mumbai, Maharashtra, India', 'projects': 'ðŸ‘‰Customer Behavior Modeling:\\nEstimating Customer Lifetime Value (CLV) predicts customer worth, churn forecasts retention, and purchase propensity gauges transaction counts and spending. Tier transition analysis influences segmentation, retention, and personalized marketing, enhancing satisfaction, loyalty, and revenue.\\n\\nðŸ‘‰Hashtag Search:\\nEnhancing search functionality by integrating product attributes into phrases for a more intuitive and efficient product discovery process.\\n\\nðŸ‘‰Automated Customer Feedback Report:\\nUtilizing fine-tuning of a Large Language Model to generate diverse supervised data, addressing imbalances in actual data for improved analysis and data-driven decision-making in customer satisfaction and issue resolution.\\n\\nðŸ‘‰Meta Tag Generator for PLP, PDP, and Query Pages:\\nThe \"Meta Tag Generator\" project automates meta tag creation for PLP, PDP, and Query Pages, optimizing search engine visibility and attracting targeted traffic to the website.'}, {'designation': 'Associate Vice President', 'duration': 'Apr 2021 to Oct 2023 Â· 2 yrs 7 mos', 'location': 'Mumbai, Maharashtra, India Â· On-site', 'projects': 'ðŸ‘‰Personalized Search:\\nEngineered dynamic product ranking tailored to individual users, optimizing the search experience based on user preferences.\\n\\nðŸ‘‰Graph and Big Data Analysis:\\nDesigned and implemented graph and big-data analysis tools to extract valuable insights, contributing to strategic decision-making.\\n\\nðŸ‘‰Implicit Customer Features Identification:\\nUtilized demographic data, including religion, native region, and gender, to drive targeted marketing, resulting in personalized recommendations and search suggestions aligned with individual preferences.\\n\\nðŸ‘‰Social Graph Analytics for Personalized Search and Recommendation:\\nAnalyzed user social graphs and click journeys to develop sophisticated recommendation systems, providing personalized suggestions based on user behavior and connections.\\n\\nðŸ‘‰Customer Segmentation and Ranking:\\nImplemented techniques to segment customers based on diverse attributes and ranked them according to relevance and potential value, enhancing the personalization of product rankings.\\n\\nðŸ‘‰Image Segmentation and Phrase Generation:\\nApplied image segmentation techniques to enrich e-commerce product attributes and utilized phrase generation to create descriptive and informative content.\\n\\nðŸ‘‰Customer Purchase Intention and Cart Value Prediction:\\nDeveloped models for predicting customer purchase intentions and cart values, contributing to optimized marketing strategies and an improved shopping experience.\\n\\nðŸ‘‰User Category and MRP Affinity:\\nExplored user categories and their affinity towards specific products or price ranges, leveraging insights to enhance personalized recommendations and targeted marketing efforts.\\n\\nðŸ‘‰Social Graph-Based Occasion Marking and Relation Prediction:\\nLeveraged the social graph to identify occasions and predict user relationships, facilitating personalized recommendations aligned with specific events or social connections.'}]}, {'company_name': 'mBreath', 'duration': '3 yrs 4 mos', 'designations': [{'designation': 'Co-Founder', 'duration': 'Aug 2016 to Nov 2019 Â· 3 yrs 4 mos', 'location': 'STEP, IIT Kharagpur', 'projects': 'There are some of the project which I did at mBreath.\\nReal-Time Human Vital Parameter Estimation Using Pressure Sensor and Machine Learning\\nMeasured respiration and heart rate through an array of pressure sensors and Deep Learning (CNN, RNN-LSTM) algorithms. [Filed Patent Ref No. 201731038573]. 96% accuracy was achieved. The project had three stages 1) preprocessing with analog filters, 2) Digital Signal Processing, 3) Machine Learning model thereafter data is uploaded to the cloud for further processing.\\n\\nReal-time Classification of Environmental Sounds & Their Impacts on Sleep Quality\\nCollected audio using a microphone and Qualcommâ€™s Snapdragon processor enabled custom-built board running Android 6.0. Audio Cleaning, Feature Extraction (more than 700 features) in Temporal, Spectral, Energy, Harmonic, and Cepstral domain, Feature Selection for faster training and better accuracy and ML models (CNN, LSTM, RF, SVM) are designed to classify snoring and other sound and their impact on sleep quality.\\n\\nA Real-Time System and Methods for the Detection of Sleep Apnea and Sleep Stage\\nUsed CNN, Inception, LSTM, CNN-LSTM models to estimate Sleep Apnea and Sleep Stages using the respiration signal only. The training data was obtained from MESA, National Sleep Research Resource, USA. I did data cleaning and balancing thereafter feature engineering and selection were done before applying the ML model. Achieved 82.7% accuracy in Sleep Apnea which is higher than any known methods applied on respiration signal in MESA Dataset and achieved more than 80% accuracy in sleep stage classification which can be considered as good accuracy.\\n\\nVisit:\\nhttp://www.sleepdoc.ai\\nhttp://www.facebook.com/mBreathOfficial\\nhttp://www.twitter.com/mBreathOfficial'}]}, {'company_name': 'Indian Institute of Technology, Kharagpur', 'duration': '4 yrs 8 mos', 'designations': [{'designation': 'Student', 'duration': 'Jul 2012 to Feb 2017 Â· 4 yrs 8 mos', 'location': 'Kharagpur, India', 'projects': ''}, {'designation': 'Teacher Assistant', 'duration': 'Jan 2013 to Jan 2017 Â· 4 yrs 1 mo', 'location': 'IIT Kharagpur', 'projects': 'It was a great experience to teach Embedded Systems. The objective of the teaching is the implementation of the adaptive filter on embedded hardware and software.\\nImplementation of the DSP algorithms was done in Assembly language. Students get comprehensive knowledge of Embedded System and IoT.'}]}]\n",
      "PROJECT DONE                    =\n",
      " [{'project_name': 'Smart Sleep Sense: An Unobtrusive and IoT Enabled Wireless Sleep Monitoring System', 'duration': 'May 2015 - Present', 'description': 'The project aims at developing a portable Human monitoring device that detects the health parameters (heart pulse, breathing rate, thermal emotion recognition, awakening at REM stage of the sleep) and communicates the signals obtained from sensors to the Android app via Wireless network. The information is used to convey Smart Assistance to the user for their daily improved health care conditions thus improving their efficiency and longevity. We are offering sleep disorder monitoring devices. In addition to this, our business model is B2C and device will be directly available to the customer for purchase via online e-commerce stores as well as company e-store itself and the expected cost will be in between 110 to 149 dollar.'}, {'project_name': 'Optimized Fixed Point Kalman Filter Implementation on Embedded Hardware', 'duration': 'Aug 2013 - Apr 2014', 'description': 'Real time adaptive signal processing is an area of considerable interest for the researchers in the field of embedded signal processing. Owing to the large dynamic range and constant relative accuracy of floating point arithmetic, filters are traditionally realized in floating point data type. However, using floating point arithmetic in embedded systems lead to larger cost in terms of latency, power and memory. Therefore these algorithms need to be implemented in fixed point for optimizing these constraints. However with the reduction in the word length the precision get affected. Therefore these algorithms need to be coded properly with adequate fixed point format such that precision is not affected while the resources are optimized. In this thesis an attempt has been made to implement Least Mean Square (LMS), Recursive Least Square (RLS) and Kalman Filtering algorithms on ARM Cortex M0+ with Thumb instruction set. The word length has been optimized for accurate performance, reduced power, improved speed and reduced memory usage. The validation of the algorithm has been carried out with respect to MATLAB.'}, {'project_name': 'MIPS64 5-stages Processor Designing', 'duration': 'Jan 2013 - May 2013', 'description': 'The machine was designed with the MIPS instruction set in mind. Its purpose was to be able to take  a  binary  representation  of  a  MIPS  instruction  and  convert  it  into  microcode.    The microcode controls the actions of the processor and pipeline. Pipelining is a process of multitasking instructions in the same data path.  An instruction can be loaded into the data path at every clock cycle, even though each instruction takes up to 5 cycles to complete.  At every clock cycle, each instruction is stored into a different stage in the pipeline. Doing this does not affect any of the other instructions in the processor because they are in different stages. Each stage  contains  a  different  part  of  the  data  path  and  has  a specific function. This allows the user of the processor to fully utilize all components of the data path simultaneously, causing an increase in the throughput of their program.'}, {'project_name': 'Transistor Characteristics Curve Tracer', 'duration': 'Feb 2012 - May 2013', 'description': 'A cost effective and reliable curve tracer has been designed and developed for NPN- PNP transistors using AT89C51 microcontroller. The system plots desired number of transfer and static characteristics curves on a given range of constant biasing parameter viz; VCE and IB resp. The system plots these characteristics curves of the given transistor on the available computer system through the developed software on Visual BASIC 6.0.  h-parameters of the transistor have also been obtained for different design requirement. All the data has also been stored in a user defined output file for further analysis.'}, {'project_name': 'Automated BJT Curve Tracer using Visual Basic', 'duration': 'Jan 2012 - May 2012', 'description': 'The characteristic of any discrete component plays an important role for determining its behavior for different applications. In this paper we have developed a cost effective BJT curve tracer and binning system which consist of hardware part that is implemented using 89c51 microcontroller and software part which is developed in Microsoft Visual Basic 6.0. This complete system plots the input characteristics curves, h-parameters of the given transistor along with other parameters i.e. C-E resistance and early voltage. Binning machine is used to find the working mode of BJT and to find whether the BJT is working or not by using the port given on the hardware. All the data can also be stored in a user defined output file in the computer for further analysis. This tracer and binning machine can be used for educational and engineering purposes.'}]\n",
      "Skills done             =\n",
      " ['MLOps', 'Large Language Models (LLM)', 'PyTorch', 'Product Management', 'Cross-functional Collaborations', 'Critical Thinking', 'Predictive Modeling', 'Machine Learning', 'Artificial Intelligence (AI)', 'Deep Learning', 'Programming', 'Algorithms', 'Business Development', 'Business Strategy', 'Cloud Computing', 'Big Data', 'C', 'Matlab', 'Recurrent Neural Networks (RNN)', 'Amazon Web Services (AWS)', 'MLOps', 'Large Language Models (LLM)', 'Product Management', 'Cross-functional Collaborations', 'Predictive Modeling', 'Machine Learning', 'Artificial Intelligence (AI)', 'Deep Learning', 'Programming', 'Algorithms', 'Business Development', 'Business Strategy', 'Cloud Computing', 'Big Data', 'Convolutional Neural Networks (CNN)', 'Scikit-Learn', 'Natural Language Processing (NLP)', 'Logistic Regression', 'Random Forest', 'Segmentation', 'PyTorch', 'C', 'Matlab', 'Recurrent Neural Networks (RNN)', 'Amazon Web Services (AWS)', 'Python', 'TensorFlow', 'Keras', 'Google Cloud Platform (GCP)', 'Pandas (Software)', 'SQL', 'Critical Thinking', 'Team Leadership', 'Team Management']\n",
      "Profile_data =  {'name': 'Laxmi Kant', 'url': 'https://www.linkedin.com/in/laxmimerit', 'headline': 'Gen AI in Finance & Investment Services | Data Scientist | IIT Kharagpur | Asset Management | AI-Driven Financial Modeling | Search Ranking | NLP Python BERT AWS Elasticsearch GNN SQL LLM | AI in Investment Strategies', 'about': 'Demonstrated 8+ years of expertise in advanced analytics as an AVP in Data Science, showcasing dynamic and impactful contributions. Seeking to leverage expertise in customer behavior modeling, personalized marketing, product discovery & search optimization, and recommendations to drive impactful solutions in the fields of Data Science, Machine Learning, and Artificial Intelligence.ðŸ‘‰ Significant Achievements Across Careerâ–ª Strategically led impactful initiatives in Customer Behavior Modeling, resulting in a substantial 30% increase in customer retention and a 10% reduction in advertising spending.â–ª Pioneered the development and implementation of a Machine Learning product relevance ranking system, contributing to a remarkable 30% increase in conversion rates and a 20% growth in revenue.â–ª Applied innovative techniques, including Social Graph Analytics and Predictive  Modeling, achieving a 35% improvement in recommendation conversion rates and a 40% increase in customer care calling conversion rates.â–ª Revolutionized customer segmentation and ranking, realizing a notable 20% improvement in conversion rates.â–ª Engineered predictive models for email open rates, driving a remarkable 70% improvement in email engagement.â–ª Achieved transformative outcomes, including a substantial 50% improvement in search conversion rates, a notable 60% increase in revenue, and an impressive 90% reduction in zero-result queries.â–ª A strategic driver of user satisfaction and engagement, evidenced by a commendable 10% decrease in exit rates.Demonstrated 8+ years of expertise in advanced analytics as an AVP in Data Science, showcasing dynamic and impactful contributions. Seeking to leverage expertise in customer behavior modeling, personalized marketing, product discovery & search optimization, and recommendations to drive impactful solutions in the fields of Data Science, Machine Learning, and Artificial Intelligence.\\n\\nðŸ‘‰ Significant Achievements Across Career\\nâ–ª Strategically led impactful initiatives in Customer Behavior Modeling, resulting in a substantial 30% increase in customer retention and a 10% reduction in advertising spending.\\n\\nâ–ª Pioneered the development and implementation of a Machine Learning product relevance ranking system, contributing to a remarkable 30% increase in conversion rates and a 20% growth in revenue.\\n\\nâ–ª Applied innovative techniques, including Social Graph Analytics and Predictive  Modeling, achieving a 35% improvement in recommendation conversion rates and a 40% increase in customer care calling conversion rates.\\n\\nâ–ª Revolutionized customer segmentation and ranking, realizing a notable 20% improvement in conversion rates.\\n\\nâ–ª Engineered predictive models for email open rates, driving a remarkable 70% improvement in email engagement.\\n\\nâ–ª Achieved transformative outcomes, including a substantial 50% improvement in search conversion rates, a notable 60% increase in revenue, and an impressive 90% reduction in zero-result queries.\\n\\nâ–ª A strategic driver of user satisfaction and engagement, evidenced by a commendable 10% decrease in exit rates.', 'experience': [{'company_name': 'IGP', 'duration': 'Full-time Â· 4 yrs 10 mos', 'designations': [{'designation': 'Assistant Vice President', 'duration': 'Oct 2023 to Sep 2024 Â· 1 yr', 'location': 'Mumbai, Maharashtra, India', 'projects': 'ðŸ‘‰Customer Behavior Modeling:\\nEstimating Customer Lifetime Value (CLV) predicts customer worth, churn forecasts retention, and purchase propensity gauges transaction counts and spending. Tier transition analysis influences segmentation, retention, and personalized marketing, enhancing satisfaction, loyalty, and revenue.\\n\\nðŸ‘‰Hashtag Search:\\nEnhancing search functionality by integrating product attributes into phrases for a more intuitive and efficient product discovery process.\\n\\nðŸ‘‰Automated Customer Feedback Report:\\nUtilizing fine-tuning of a Large Language Model to generate diverse supervised data, addressing imbalances in actual data for improved analysis and data-driven decision-making in customer satisfaction and issue resolution.\\n\\nðŸ‘‰Meta Tag Generator for PLP, PDP, and Query Pages:\\nThe \"Meta Tag Generator\" project automates meta tag creation for PLP, PDP, and Query Pages, optimizing search engine visibility and attracting targeted traffic to the website.'}, {'designation': 'Associate Vice President', 'duration': 'Apr 2021 to Oct 2023 Â· 2 yrs 7 mos', 'location': 'Mumbai, Maharashtra, India Â· On-site', 'projects': 'ðŸ‘‰Personalized Search:\\nEngineered dynamic product ranking tailored to individual users, optimizing the search experience based on user preferences.\\n\\nðŸ‘‰Graph and Big Data Analysis:\\nDesigned and implemented graph and big-data analysis tools to extract valuable insights, contributing to strategic decision-making.\\n\\nðŸ‘‰Implicit Customer Features Identification:\\nUtilized demographic data, including religion, native region, and gender, to drive targeted marketing, resulting in personalized recommendations and search suggestions aligned with individual preferences.\\n\\nðŸ‘‰Social Graph Analytics for Personalized Search and Recommendation:\\nAnalyzed user social graphs and click journeys to develop sophisticated recommendation systems, providing personalized suggestions based on user behavior and connections.\\n\\nðŸ‘‰Customer Segmentation and Ranking:\\nImplemented techniques to segment customers based on diverse attributes and ranked them according to relevance and potential value, enhancing the personalization of product rankings.\\n\\nðŸ‘‰Image Segmentation and Phrase Generation:\\nApplied image segmentation techniques to enrich e-commerce product attributes and utilized phrase generation to create descriptive and informative content.\\n\\nðŸ‘‰Customer Purchase Intention and Cart Value Prediction:\\nDeveloped models for predicting customer purchase intentions and cart values, contributing to optimized marketing strategies and an improved shopping experience.\\n\\nðŸ‘‰User Category and MRP Affinity:\\nExplored user categories and their affinity towards specific products or price ranges, leveraging insights to enhance personalized recommendations and targeted marketing efforts.\\n\\nðŸ‘‰Social Graph-Based Occasion Marking and Relation Prediction:\\nLeveraged the social graph to identify occasions and predict user relationships, facilitating personalized recommendations aligned with specific events or social connections.'}]}, {'company_name': 'mBreath', 'duration': '3 yrs 4 mos', 'designations': [{'designation': 'Co-Founder', 'duration': 'Aug 2016 to Nov 2019 Â· 3 yrs 4 mos', 'location': 'STEP, IIT Kharagpur', 'projects': 'There are some of the project which I did at mBreath.\\nReal-Time Human Vital Parameter Estimation Using Pressure Sensor and Machine Learning\\nMeasured respiration and heart rate through an array of pressure sensors and Deep Learning (CNN, RNN-LSTM) algorithms. [Filed Patent Ref No. 201731038573]. 96% accuracy was achieved. The project had three stages 1) preprocessing with analog filters, 2) Digital Signal Processing, 3) Machine Learning model thereafter data is uploaded to the cloud for further processing.\\n\\nReal-time Classification of Environmental Sounds & Their Impacts on Sleep Quality\\nCollected audio using a microphone and Qualcommâ€™s Snapdragon processor enabled custom-built board running Android 6.0. Audio Cleaning, Feature Extraction (more than 700 features) in Temporal, Spectral, Energy, Harmonic, and Cepstral domain, Feature Selection for faster training and better accuracy and ML models (CNN, LSTM, RF, SVM) are designed to classify snoring and other sound and their impact on sleep quality.\\n\\nA Real-Time System and Methods for the Detection of Sleep Apnea and Sleep Stage\\nUsed CNN, Inception, LSTM, CNN-LSTM models to estimate Sleep Apnea and Sleep Stages using the respiration signal only. The training data was obtained from MESA, National Sleep Research Resource, USA. I did data cleaning and balancing thereafter feature engineering and selection were done before applying the ML model. Achieved 82.7% accuracy in Sleep Apnea which is higher than any known methods applied on respiration signal in MESA Dataset and achieved more than 80% accuracy in sleep stage classification which can be considered as good accuracy.\\n\\nVisit:\\nhttp://www.sleepdoc.ai\\nhttp://www.facebook.com/mBreathOfficial\\nhttp://www.twitter.com/mBreathOfficial'}]}, {'company_name': 'Indian Institute of Technology, Kharagpur', 'duration': '4 yrs 8 mos', 'designations': [{'designation': 'Student', 'duration': 'Jul 2012 to Feb 2017 Â· 4 yrs 8 mos', 'location': 'Kharagpur, India', 'projects': ''}, {'designation': 'Teacher Assistant', 'duration': 'Jan 2013 to Jan 2017 Â· 4 yrs 1 mo', 'location': 'IIT Kharagpur', 'projects': 'It was a great experience to teach Embedded Systems. The objective of the teaching is the implementation of the adaptive filter on embedded hardware and software.\\nImplementation of the DSP algorithms was done in Assembly language. Students get comprehensive knowledge of Embedded System and IoT.'}]}], 'projects': [{'project_name': 'Smart Sleep Sense: An Unobtrusive and IoT Enabled Wireless Sleep Monitoring System', 'duration': 'May 2015 - Present', 'description': 'The project aims at developing a portable Human monitoring device that detects the health parameters (heart pulse, breathing rate, thermal emotion recognition, awakening at REM stage of the sleep) and communicates the signals obtained from sensors to the Android app via Wireless network. The information is used to convey Smart Assistance to the user for their daily improved health care conditions thus improving their efficiency and longevity. We are offering sleep disorder monitoring devices. In addition to this, our business model is B2C and device will be directly available to the customer for purchase via online e-commerce stores as well as company e-store itself and the expected cost will be in between 110 to 149 dollar.'}, {'project_name': 'Optimized Fixed Point Kalman Filter Implementation on Embedded Hardware', 'duration': 'Aug 2013 - Apr 2014', 'description': 'Real time adaptive signal processing is an area of considerable interest for the researchers in the field of embedded signal processing. Owing to the large dynamic range and constant relative accuracy of floating point arithmetic, filters are traditionally realized in floating point data type. However, using floating point arithmetic in embedded systems lead to larger cost in terms of latency, power and memory. Therefore these algorithms need to be implemented in fixed point for optimizing these constraints. However with the reduction in the word length the precision get affected. Therefore these algorithms need to be coded properly with adequate fixed point format such that precision is not affected while the resources are optimized. In this thesis an attempt has been made to implement Least Mean Square (LMS), Recursive Least Square (RLS) and Kalman Filtering algorithms on ARM Cortex M0+ with Thumb instruction set. The word length has been optimized for accurate performance, reduced power, improved speed and reduced memory usage. The validation of the algorithm has been carried out with respect to MATLAB.'}, {'project_name': 'MIPS64 5-stages Processor Designing', 'duration': 'Jan 2013 - May 2013', 'description': 'The machine was designed with the MIPS instruction set in mind. Its purpose was to be able to take  a  binary  representation  of  a  MIPS  instruction  and  convert  it  into  microcode.    The microcode controls the actions of the processor and pipeline. Pipelining is a process of multitasking instructions in the same data path.  An instruction can be loaded into the data path at every clock cycle, even though each instruction takes up to 5 cycles to complete.  At every clock cycle, each instruction is stored into a different stage in the pipeline. Doing this does not affect any of the other instructions in the processor because they are in different stages. Each stage  contains  a  different  part  of  the  data  path  and  has  a specific function. This allows the user of the processor to fully utilize all components of the data path simultaneously, causing an increase in the throughput of their program.'}, {'project_name': 'Transistor Characteristics Curve Tracer', 'duration': 'Feb 2012 - May 2013', 'description': 'A cost effective and reliable curve tracer has been designed and developed for NPN- PNP transistors using AT89C51 microcontroller. The system plots desired number of transfer and static characteristics curves on a given range of constant biasing parameter viz; VCE and IB resp. The system plots these characteristics curves of the given transistor on the available computer system through the developed software on Visual BASIC 6.0.  h-parameters of the transistor have also been obtained for different design requirement. All the data has also been stored in a user defined output file for further analysis.'}, {'project_name': 'Automated BJT Curve Tracer using Visual Basic', 'duration': 'Jan 2012 - May 2012', 'description': 'The characteristic of any discrete component plays an important role for determining its behavior for different applications. In this paper we have developed a cost effective BJT curve tracer and binning system which consist of hardware part that is implemented using 89c51 microcontroller and software part which is developed in Microsoft Visual Basic 6.0. This complete system plots the input characteristics curves, h-parameters of the given transistor along with other parameters i.e. C-E resistance and early voltage. Binning machine is used to find the working mode of BJT and to find whether the BJT is working or not by using the port given on the hardware. All the data can also be stored in a user defined output file in the computer for further analysis. This tracer and binning machine can be used for educational and engineering purposes.'}], 'skills': ['MLOps', 'Large Language Models (LLM)', 'PyTorch', 'Product Management', 'Cross-functional Collaborations', 'Critical Thinking', 'Predictive Modeling', 'Machine Learning', 'Artificial Intelligence (AI)', 'Deep Learning', 'Programming', 'Algorithms', 'Business Development', 'Business Strategy', 'Cloud Computing', 'Big Data', 'C', 'Matlab', 'Recurrent Neural Networks (RNN)', 'Amazon Web Services (AWS)', 'MLOps', 'Large Language Models (LLM)', 'Product Management', 'Cross-functional Collaborations', 'Predictive Modeling', 'Machine Learning', 'Artificial Intelligence (AI)', 'Deep Learning', 'Programming', 'Algorithms', 'Business Development', 'Business Strategy', 'Cloud Computing', 'Big Data', 'Convolutional Neural Networks (CNN)', 'Scikit-Learn', 'Natural Language Processing (NLP)', 'Logistic Regression', 'Random Forest', 'Segmentation', 'PyTorch', 'C', 'Matlab', 'Recurrent Neural Networks (RNN)', 'Amazon Web Services (AWS)', 'Python', 'TensorFlow', 'Keras', 'Google Cloud Platform (GCP)', 'Pandas (Software)', 'SQL', 'Critical Thinking', 'Team Leadership', 'Team Management']}\n",
      "Profile data saved to profile_data.json\n"
     ]
    }
   ],
   "source": [
    "def login():\n",
    "    driver.get('https://www.linkedin.com/login')\n",
    "    try:\n",
    "        wait.until(EC.title_contains(\"LinkedIn Login\"))\n",
    "    except TimeoutException:\n",
    "        print(\"Error: Not on login page\")\n",
    "        return\n",
    "\n",
    "    email = wait.until(EC.presence_of_element_located((By.ID, 'username')))\n",
    "    email.send_keys(os.environ['EMAIL'])\n",
    "\n",
    "    password = driver.find_element(By.ID, 'password')\n",
    "    password.send_keys(os.environ['PASSWORD'])\n",
    "    password.submit()\n",
    "\n",
    "    try:\n",
    "        wait.until(EC.url_contains(\"/feed/\"))\n",
    "    except TimeoutException:\n",
    "        print(\"Error: Login failed or page not redirected\")\n",
    "\n",
    "def scrape_name_headline():\n",
    "    driver.get(url)\n",
    "    sleep(4)\n",
    "    try:\n",
    "        wait.until(EC.presence_of_element_located((By.XPATH, \"//h1[contains(@class, 'inline') and contains(@class, 't-24') and contains(@class, 'v-align-middle')]\")))\n",
    "    except TimeoutException:\n",
    "        print(\"Error: Could not load profile page or invalid URL\")\n",
    "        return\n",
    "\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'lxml')\n",
    "\n",
    "    try:\n",
    "        name_element = driver.find_element(By.XPATH, \"//h1[contains(@class, 'inline') and contains(@class, 't-24') and contains(@class, 'v-align-middle')]\")\n",
    "        name = name_element.text.strip()\n",
    "        profile_data['name'] = name\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        profile_data['name'] = \"\"\n",
    "\n",
    "    profile_data['url'] = url\n",
    "\n",
    "    try:\n",
    "        headline = soup.find('div', {'class': 'text-body-medium break-words'})\n",
    "        profile_data['headline'] = headline.get_text().strip() if headline else \"\"\n",
    "        print(\"Headline fetched \", profile_data)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        profile_data['headline'] = \"\"\n",
    "\n",
    "def get_all_sections_list():\n",
    "    try:\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'lxml')\n",
    "        sleep(1)\n",
    "        return soup.find_all('section', {'class': 'artdeco-card pv-profile-card break-words mt2'})\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return\n",
    "\n",
    "def scrape_about(sections):\n",
    "    try:\n",
    "        about_section = next((sec for sec in sections if sec.find('div', {'id': 'about'})), None)\n",
    "        #sleep(1)\n",
    "        if not about_section:\n",
    "            print(\"No About section found\")\n",
    "            profile_data['about'] = \"\"\n",
    "            return\n",
    "        try:\n",
    "            about = about_section.find('div', class_='display-flex ph5 pv3')\n",
    "            profile_data['about'] = about.get_text().strip() if about else \"\"\n",
    "            print('ABOUT DONE                  = \\n', profile_data['about'])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            profile_data['about'] = \"\"\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        profile_data['about'] = \"\"\n",
    "    \n",
    "def get_exp(exp):\n",
    "    exp_dict = {}\n",
    "\n",
    "    # Extract company name\n",
    "    try:\n",
    "        name_container = exp.find('div', {'class': 'display-flex flex-wrap align-items-center full-height'})\n",
    "        name = name_container.find('span', {'class': 'visually-hidden'}).get_text().strip() if name_container else \"\"\n",
    "    except AttributeError:\n",
    "        name = \"\"\n",
    "    exp_dict['company_name'] = name\n",
    "\n",
    "    # Extract duration\n",
    "    try:\n",
    "        duration_container = exp.find('span', {'class': 't-14 t-normal'})\n",
    "        duration = duration_container.find('span', {'class': 'visually-hidden'}).get_text().strip() if duration_container else \"\"\n",
    "    except AttributeError:\n",
    "        duration = \"\"\n",
    "    exp_dict['duration'] = duration\n",
    "\n",
    "    # Extract designations\n",
    "    designations = exp.find_all('div', {'class': 'fPLNkfiTqBJivqMiXLaRuObcmlUMZsDPkIAVk yCpXOOXwXcJnFOsCoYTsmMdAvLcplVbgNCBU'}) or []\n",
    "    item_list = []\n",
    "\n",
    "    for position in designations:\n",
    "        spans = position.find_all('span', {'class': 'visually-hidden'})\n",
    "        item_dict = {\n",
    "            'designation': spans[0].get_text().strip() if len(spans) > 0 else \"\",\n",
    "            'duration': spans[1].get_text().strip() if len(spans) > 1 else \"\",\n",
    "            'location': spans[2].get_text().strip() if len(spans) > 2 else \"\",\n",
    "            'projects': spans[3].get_text().strip() if len(spans) > 3 else \"\"\n",
    "        }\n",
    "        item_list.append(item_dict)\n",
    "\n",
    "    exp_dict['designations'] = item_list\n",
    "\n",
    "    return exp_dict\n",
    "\n",
    "\n",
    "def scrape_experience(sections):\n",
    "    try:\n",
    "        experience_section = next((sec for sec in sections if sec.find('div', {'id': 'experience'})), None)\n",
    "        sleep(1)\n",
    "        if not experience_section:\n",
    "            print(\"No Experience section found\")\n",
    "            profile_data['experience'] = []\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            experiences = experience_section.find_all('div', {'class': 'fPLNkfiTqBJivqMiXLaRuObcmlUMZsDPkIAVk SeRILEEOfWLelfcuceiLywOjAamlMoEkmnTdFk itGgYIXPpfAaqNrUDXHQVkGSUXMzldwQtdzM'})\n",
    "            profile_data['experience'] = [get_exp(exp) for exp in experiences]\n",
    "            print('EXPERIENCE DONE                   =\\n',profile_data['experience'])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            profile_data['experience'] = []\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        profile_data['experience'] = []\n",
    "\n",
    "\n",
    "def get_project(item):\n",
    "    spans = item.find_all('span', {'class': 'visually-hidden'})\n",
    "\n",
    "    item_dict = {\n",
    "        'project_name': spans[0].get_text().strip() if len(spans) > 0 else \"\",\n",
    "        'duration': spans[1].get_text().strip() if len(spans) > 1 else \"\",\n",
    "        'description': spans[2].get_text().strip() if len(spans) > 2 else \"\",\n",
    "    }\n",
    "    return item_dict\n",
    "\n",
    "\n",
    "def scrape_projects():\n",
    "    try:\n",
    "        # Wait for the 'See All Projects' button and click it\n",
    "        view_all_projects_button = wait.until(EC.element_to_be_clickable((By.ID, \"navigation-index-see-all-projects\")))\n",
    "        view_all_projects_button.click()\n",
    "        sleep(4)\n",
    "    except TimeoutException:\n",
    "        print(\"Failed to find the 'See All Projects' button.\")\n",
    "        profile_data['projects'] = []\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Wait for the projects section to load\n",
    "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'section.artdeco-card.pb3')))\n",
    "    except TimeoutException:\n",
    "        print(\"Failed to load the projects section.\")\n",
    "        profile_data['projects'] = []\n",
    "        return\n",
    "\n",
    "    # Parse the page source for project details\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'lxml')\n",
    "\n",
    "    projects_section = soup.find('section', {'class': 'artdeco-card pb3'})\n",
    "    items = projects_section.find_all('div', {'class': 'fPLNkfiTqBJivqMiXLaRuObcmlUMZsDPkIAVk SeRILEEOfWLelfcuceiLywOjAamlMoEkmnTdFk itGgYIXPpfAaqNrUDXHQVkGSUXMzldwQtdzM'}) if projects_section else []\n",
    "\n",
    "    profile_data['projects'] = [get_project(item) for item in items]\n",
    "    print('PROJECT DONE                    =\\n',profile_data['projects'])\n",
    "\n",
    "    # Navigate back to the main profile page\n",
    "    driver.back()\n",
    "    sleep(4)\n",
    "    #wait.until(EC.url_to_be(url))  # Ensure the navigation is complete\n",
    "\n",
    "\n",
    "def get_skills(item):\n",
    "    spans = item.find_all('span', {'class': 'visually-hidden'})\n",
    "    return spans[0].get_text().strip() if spans else \"\"\n",
    "\n",
    "\n",
    "def scrape_skills():\n",
    "    try:\n",
    "        # Wait for the 'Show All Skills' link and click it\n",
    "        view_all_skills_link = wait.until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//a[contains(@id, 'navigation-index-Show-all-') and contains(@id, '-skills')]\"))\n",
    "        )\n",
    "        view_all_skills_link.click()\n",
    "        sleep(4)\n",
    "    except TimeoutException:\n",
    "        print(\"Failed to find or click the 'Show All Skills' link.\")\n",
    "        profile_data['skills'] = []\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Wait for the skills section to load\n",
    "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'section.artdeco-card.pb3')))\n",
    "    except TimeoutException:\n",
    "        print(\"Failed to load the skills section.\")\n",
    "        profile_data['skills'] = []\n",
    "        return\n",
    "\n",
    "    # Parse the page source for skills\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'lxml')\n",
    "\n",
    "    skills_section = soup.find('section', {'class': 'artdeco-card pb3'})\n",
    "    items = skills_section.find_all('div', {'class': 'fPLNkfiTqBJivqMiXLaRuObcmlUMZsDPkIAVk SeRILEEOfWLelfcuceiLywOjAamlMoEkmnTdFk itGgYIXPpfAaqNrUDXHQVkGSUXMzldwQtdzM'}) if skills_section else []\n",
    "\n",
    "    profile_data['skills'] = [get_skills(item) for item in items]\n",
    "    print('Skills done             =\\n',profile_data['skills'])\n",
    "\n",
    "    # Navigate back to the main profile page\n",
    "    driver.back()\n",
    "    sleep(4)\n",
    "    #wait.until(EC.url_to_be(url))  # Ensure the navigation is complete\n",
    "\n",
    "import json\n",
    "def save_profile_data_to_json(profile_data, file_name=\"profile_data.json\"):\n",
    "    try:\n",
    "        # Convert Python dictionary to a JSON string and save it to a file\n",
    "        with open(file_name, 'w', encoding='utf-8') as json_file:\n",
    "            json.dump(profile_data, json_file, indent=4, ensure_ascii=False)\n",
    "        print(f\"Profile data saved to {file_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while saving JSON: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "# Main execution flow\n",
    "sections = None\n",
    "\n",
    "login()\n",
    "scrape_name_headline()\n",
    "sections = get_all_sections_list()\n",
    "#print('sections = ',sections)\n",
    "scrape_about(sections)\n",
    "scrape_experience(sections)\n",
    "scrape_projects()\n",
    "scrape_skills()\n",
    "print(\"Profile_data = \", profile_data)\n",
    "driver.quit()\n",
    "\n",
    "save_profile_data_to_json(profile_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####################################################################################################################################################################################################################################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headline fetched  {'name': 'Siddhartha Chandra', 'url': 'https://www.linkedin.com/in/chandrasiddhartha/', 'headline': 'Technologist | Startup & Student Mentor | EdTech Innovator | Educator & Community Builder'}\n",
      "ABOUT DONE                  = \n",
      " Seasoned developer in Data Science, with a diverse experience of solving problems in several US-based AI start-ups. I'm passionate about solving problems collaboratively by building resourceful solutions on robust and flexible architectural foundations, which can scale organically accruing minimal tech debt.Experience: 8+ years in Data Science11+ years in EngineeringIn my past roles, I have owned and built multiple test-backed systems from ground-up taking them from prototype to production, transitioned legacy systems, created insightful monitoring systems, authored internal libraries and data connection layers, written and maintained data pipelines in Airflow and CI/CD systems in Jenkins, and mentored developers.Areas of Expertise:Programming: Python, ScalaDatabases: Hive, SQL, MariaDB, Mongo, Elasticsearch, Postgres, Redis, WeaviateFrameworks: Flask, DashML: Classification, Regression, Clustering, Outlier Detection, NLP, Time-series forecasting, Sentence Transformers, automlVisualisation: plotly, seaborn, matplotlibMonitoring: Grafana, LibratoBig Data: Apache Spark, HDFSWorkflow management: Airflow, KafkaIDE's: Pycharm, Intellij, Datagrip, VSCodeSoft skills: Communication, Listening, Presentation, Leadership, Retrospectives, Team development, InterviewingSeasoned developer in Data Science, with a diverse experience of solving problems in several US-based AI start-ups. I'm passionate about solving problems collaboratively by building resourceful solutions on robust and flexible architectural foundations, which can scale organically accruing minimal tech debt.\n",
      "\n",
      "Experience: \n",
      "8+ years in Data Science\n",
      "11+ years in Engineering\n",
      "\n",
      "In my past roles, I have owned and built multiple test-backed systems from ground-up taking them from prototype to production, transitioned legacy systems, created insightful monitoring systems, authored internal libraries and data connection layers, written and maintained data pipelines in Airflow and CI/CD systems in Jenkins, and mentored developers.\n",
      "\n",
      "\n",
      "Areas of Expertise:\n",
      "\n",
      "Programming: Python, Scala\n",
      "Databases: Hive, SQL, MariaDB, Mongo, Elasticsearch, Postgres, Redis, Weaviate\n",
      "Frameworks: Flask, Dash\n",
      "ML: Classification, Regression, Clustering, Outlier Detection, NLP, Time-series forecasting, Sentence Transformers, automl\n",
      "Visualisation: plotly, seaborn, matplotlib\n",
      "Monitoring: Grafana, Librato\n",
      "Big Data: Apache Spark, HDFS\n",
      "Workflow management: Airflow, Kafka\n",
      "IDE's: Pycharm, Intellij, Datagrip, VSCode\n",
      "Soft skills: Communication, Listening, Presentation, Leadership, Retrospectives, Team development, Interviewing\n",
      "\n",
      " \n",
      "              â€¦see more\n",
      "EXPERIENCE DONE                   =\n",
      " [{'company_name': 'Chief Executive Officer', 'duration': 'SP-TBI Â· Full-time', 'logo': 'https://media.licdn.com/dms/image/v2/C560BAQEKG5x_yU0olg/company-logo_100_100/company-logo_100_100/0/1631404661204/sp_tbi_logo?e=1741824000&v=beta&t=ZFFhVE31l1YSHL0heHc8LpK3k-WePk9IpOcwramALXU', 'designations': []}, {'company_name': 'Professor of Practice', 'duration': 'Bhartiya Vidya Bhavans Sardar Patel Institute of Technology Munshi Nagar Andheri Mumbai Â· Part-time', 'logo': 'https://media.licdn.com/dms/image/v2/C510BAQHE1bLMZIcIxw/company-logo_100_100/company-logo_100_100/0/1630567177590/bhartiya_vidya_bhavans_sardar_patel_institute_of_technology_munshi_nagar_andheri_mumbai_logo?e=1741824000&v=beta&t=Z7IrGy9uI8Pib72_NBSInOhZqhWUh29joKh4S8XwpbM', 'designations': []}, {'company_name': 'Founder', 'duration': 'Productivity stealth startup Â· Self-employed', 'logo': '', 'designations': []}, {'company_name': 'Lead Machine Learning Engineer', 'duration': 'NNext Â· Full-time', 'logo': 'https://media.licdn.com/dms/image/v2/D560BAQGC2UfHy7FIKw/company-logo_100_100/company-logo_100_100/0/1698971567557/nnext_co_logo?e=1741824000&v=beta&t=BrKsizSdrMhvzQ__NXBMZ2LNd_OsMfgaeG6Yg7tNlYg', 'designations': []}, {'company_name': 'Data Scientist', 'duration': 'Collective[i]', 'logo': 'https://media.licdn.com/dms/image/v2/D560BAQG1wEU0SSGUVQ/company-logo_100_100/company-logo_100_100/0/1720398339018/collective_i__logo?e=1741824000&v=beta&t=ZyxQTXPQRTDthP1SL50k7dM_codK9OZuD2x1YX0q-ng', 'designations': []}]\n",
      "Failed to find the 'See All Projects' button.\n",
      "Skills done             =\n",
      " ['Business Analytics', 'Teaching', 'Analytical Skills', 'Problem Analysis', 'Functional Programming', 'ChatGPT', 'Artificial Intelligence (AI)', 'Natural Language Processing (NLP)', 'Data Visualization', 'Data Analysis', 'Big Data', 'Jenkins', 'Programming', 'Data Structures', 'Identity & Access Management (IAM)', 'Software Development', 'Hadoop', 'Git', 'SQL', 'Apache Spark', 'Business Analytics', 'Teaching', 'Functional Programming', 'Artificial Intelligence (AI)', 'Natural Language Processing (NLP)', 'Data Visualization', 'Data Analysis', 'Big Data', 'Programming', 'Data Structures', 'Identity & Access Management (IAM)', 'Software Development', 'Apache Spark', 'Object-Oriented Programming (OOP)', 'Digital Image Processing', 'Machine Learning', 'Computer Vision', 'Computer Graphics', 'iOS development', 'Security Architecture Design', 'Jenkins', 'Hadoop', 'Git', 'SQL', 'Python (Programming Language)', 'C++', 'Scala', 'MongoDB', 'Python', 'Matlab', 'Java', 'OpenGL', 'PL/SQL', 'Unix', 'Eclipse', 'Microsoft SQL Server', 'WebSphere Application Server', 'Weblogic', 'ASP.NET', 'VMware ESX', 'Analytical Skills', 'Problem Analysis', 'Intrapersonal Skills', 'ChatGPT']\n",
      "Profile_data =  {'name': 'Siddhartha Chandra', 'url': 'https://www.linkedin.com/in/chandrasiddhartha/', 'headline': 'Technologist | Startup & Student Mentor | EdTech Innovator | Educator & Community Builder', 'pfp': 'https://media.licdn.com/dms/image/v2/D4D03AQFWg2jU8MfSag/profile-displayphoto-shrink_400_400/profile-displayphoto-shrink_400_400/0/1681301452770?e=1739404800&v=beta&t=wpJxy4ufWj-oYvDg1EIyB2S0WWSp0n2QGdet5rMe_l0', 'banner': 'https://media.licdn.com/dms/image/v2/D4D16AQGvNQz8YAvH9Q/profile-displaybackgroundimage-shrink_350_1400/profile-displaybackgroundimage-shrink_350_1400/0/1678084310146?e=1739404800&v=beta&t=X24wkVHXPVoXUVTF0lGjjPBLFA8m0oYIif9DWOuPDvg', 'about': \"Seasoned developer in Data Science, with a diverse experience of solving problems in several US-based AI start-ups. I'm passionate about solving problems collaboratively by building resourceful solutions on robust and flexible architectural foundations, which can scale organically accruing minimal tech debt.Experience: 8+ years in Data Science11+ years in EngineeringIn my past roles, I have owned and built multiple test-backed systems from ground-up taking them from prototype to production, transitioned legacy systems, created insightful monitoring systems, authored internal libraries and data connection layers, written and maintained data pipelines in Airflow and CI/CD systems in Jenkins, and mentored developers.Areas of Expertise:Programming: Python, ScalaDatabases: Hive, SQL, MariaDB, Mongo, Elasticsearch, Postgres, Redis, WeaviateFrameworks: Flask, DashML: Classification, Regression, Clustering, Outlier Detection, NLP, Time-series forecasting, Sentence Transformers, automlVisualisation: plotly, seaborn, matplotlibMonitoring: Grafana, LibratoBig Data: Apache Spark, HDFSWorkflow management: Airflow, KafkaIDE's: Pycharm, Intellij, Datagrip, VSCodeSoft skills: Communication, Listening, Presentation, Leadership, Retrospectives, Team development, InterviewingSeasoned developer in Data Science, with a diverse experience of solving problems in several US-based AI start-ups. I'm passionate about solving problems collaboratively by building resourceful solutions on robust and flexible architectural foundations, which can scale organically accruing minimal tech debt.\\n\\nExperience: \\n8+ years in Data Science\\n11+ years in Engineering\\n\\nIn my past roles, I have owned and built multiple test-backed systems from ground-up taking them from prototype to production, transitioned legacy systems, created insightful monitoring systems, authored internal libraries and data connection layers, written and maintained data pipelines in Airflow and CI/CD systems in Jenkins, and mentored developers.\\n\\n\\nAreas of Expertise:\\n\\nProgramming: Python, Scala\\nDatabases: Hive, SQL, MariaDB, Mongo, Elasticsearch, Postgres, Redis, Weaviate\\nFrameworks: Flask, Dash\\nML: Classification, Regression, Clustering, Outlier Detection, NLP, Time-series forecasting, Sentence Transformers, automl\\nVisualisation: plotly, seaborn, matplotlib\\nMonitoring: Grafana, Librato\\nBig Data: Apache Spark, HDFS\\nWorkflow management: Airflow, Kafka\\nIDE's: Pycharm, Intellij, Datagrip, VSCode\\nSoft skills: Communication, Listening, Presentation, Leadership, Retrospectives, Team development, Interviewing\\n\\n \\n              â€¦see more\", 'experience': [{'company_name': 'Chief Executive Officer', 'duration': 'SP-TBI Â· Full-time', 'logo': 'https://media.licdn.com/dms/image/v2/C560BAQEKG5x_yU0olg/company-logo_100_100/company-logo_100_100/0/1631404661204/sp_tbi_logo?e=1741824000&v=beta&t=ZFFhVE31l1YSHL0heHc8LpK3k-WePk9IpOcwramALXU', 'designations': []}, {'company_name': 'Professor of Practice', 'duration': 'Bhartiya Vidya Bhavans Sardar Patel Institute of Technology Munshi Nagar Andheri Mumbai Â· Part-time', 'logo': 'https://media.licdn.com/dms/image/v2/C510BAQHE1bLMZIcIxw/company-logo_100_100/company-logo_100_100/0/1630567177590/bhartiya_vidya_bhavans_sardar_patel_institute_of_technology_munshi_nagar_andheri_mumbai_logo?e=1741824000&v=beta&t=Z7IrGy9uI8Pib72_NBSInOhZqhWUh29joKh4S8XwpbM', 'designations': []}, {'company_name': 'Founder', 'duration': 'Productivity stealth startup Â· Self-employed', 'logo': '', 'designations': []}, {'company_name': 'Lead Machine Learning Engineer', 'duration': 'NNext Â· Full-time', 'logo': 'https://media.licdn.com/dms/image/v2/D560BAQGC2UfHy7FIKw/company-logo_100_100/company-logo_100_100/0/1698971567557/nnext_co_logo?e=1741824000&v=beta&t=BrKsizSdrMhvzQ__NXBMZ2LNd_OsMfgaeG6Yg7tNlYg', 'designations': []}, {'company_name': 'Data Scientist', 'duration': 'Collective[i]', 'logo': 'https://media.licdn.com/dms/image/v2/D560BAQG1wEU0SSGUVQ/company-logo_100_100/company-logo_100_100/0/1720398339018/collective_i__logo?e=1741824000&v=beta&t=ZyxQTXPQRTDthP1SL50k7dM_codK9OZuD2x1YX0q-ng', 'designations': []}], 'projects': [], 'skills': ['Business Analytics', 'Teaching', 'Analytical Skills', 'Problem Analysis', 'Functional Programming', 'ChatGPT', 'Artificial Intelligence (AI)', 'Natural Language Processing (NLP)', 'Data Visualization', 'Data Analysis', 'Big Data', 'Jenkins', 'Programming', 'Data Structures', 'Identity & Access Management (IAM)', 'Software Development', 'Hadoop', 'Git', 'SQL', 'Apache Spark', 'Business Analytics', 'Teaching', 'Functional Programming', 'Artificial Intelligence (AI)', 'Natural Language Processing (NLP)', 'Data Visualization', 'Data Analysis', 'Big Data', 'Programming', 'Data Structures', 'Identity & Access Management (IAM)', 'Software Development', 'Apache Spark', 'Object-Oriented Programming (OOP)', 'Digital Image Processing', 'Machine Learning', 'Computer Vision', 'Computer Graphics', 'iOS development', 'Security Architecture Design', 'Jenkins', 'Hadoop', 'Git', 'SQL', 'Python (Programming Language)', 'C++', 'Scala', 'MongoDB', 'Python', 'Matlab', 'Java', 'OpenGL', 'PL/SQL', 'Unix', 'Eclipse', 'Microsoft SQL Server', 'WebSphere Application Server', 'Weblogic', 'ASP.NET', 'VMware ESX', 'Analytical Skills', 'Problem Analysis', 'Intrapersonal Skills', 'ChatGPT']}\n",
      "Profile data saved to profile_data.json\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import sys\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "\n",
    "#url = \"https://www.linkedin.com/in/laxmimerit\"  # profile to scrape\n",
    "global driver\n",
    "global wait\n",
    "global url\n",
    "global profile_data\n",
    "\n",
    "def login():\n",
    "    driver.get('https://www.linkedin.com/login')\n",
    "    try:\n",
    "        wait.until(EC.title_contains(\"LinkedIn Login\"))\n",
    "    except TimeoutException:\n",
    "        print(\"Error: Not on login page\")\n",
    "        return\n",
    "\n",
    "    email = wait.until(EC.presence_of_element_located((By.ID, 'username')))\n",
    "    email.send_keys(os.environ['EMAIL'])\n",
    "\n",
    "    password = driver.find_element(By.ID, 'password')\n",
    "    password.send_keys(os.environ['PASSWORD'])\n",
    "    password.submit()\n",
    "\n",
    "    try:\n",
    "        wait.until(EC.url_contains(\"/feed/\"))\n",
    "    except TimeoutException:\n",
    "        print(\"Error: Login failed or page not redirected\")\n",
    "\n",
    "def scrape_name_headline():\n",
    "    driver.get(url)\n",
    "    sleep(4)\n",
    "    try:\n",
    "        wait.until(EC.presence_of_element_located((By.XPATH, \"//h1[contains(@class, 'inline') and contains(@class, 't-24') and contains(@class, 'v-align-middle')]\")))\n",
    "    except TimeoutException:\n",
    "        print(\"Error: Could not load profile page or invalid URL\")\n",
    "\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'lxml')\n",
    "\n",
    "    try:\n",
    "        name_element = driver.find_element(By.XPATH, \"//h1[contains(@class, 'inline') and contains(@class, 't-24') and contains(@class, 'v-align-middle')]\")\n",
    "        name = name_element.text.strip()\n",
    "        profile_data['name'] = name\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        profile_data['name'] = \"\"\n",
    "\n",
    "    profile_data['url'] = url\n",
    "\n",
    "    try:\n",
    "        headline = soup.find('div', {'class': 'text-body-medium break-words'})\n",
    "        profile_data['headline'] = headline.get_text().strip() if headline else \"\"\n",
    "        print(\"Headline fetched \", profile_data)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        profile_data['headline'] = \"\"\n",
    "\n",
    "def scrape_pfp_banner():\n",
    "    try:\n",
    "        page_source = driver.page_source  # gets all the html code\n",
    "        soup = BeautifulSoup(page_source, 'lxml') # parse\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    try:\n",
    "        image_div = soup.find('div', {'class': 'ph5 pb5'})\n",
    "        image_tag = image_div.find('img', {'class' : 'gYwGeQHVKFihyyWibCvmHZFDQZfKneaBo pv-top-card-profile-picture__image--show evi-image ember-view'})\n",
    "        pfp_uri = image_tag.get('src')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pfp_uri=\"\"\n",
    "    profile_data['pfp'] = pfp_uri\n",
    "\n",
    "    try:\n",
    "        image_tag = soup.find('img', {'id': \"profile-background-image-target-image\"})\n",
    "        banner_uri = image_tag.get('src')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        banner_uri = \"\"\n",
    "    profile_data['banner'] = banner_uri\n",
    "    \n",
    "\n",
    "def get_all_sections_list():\n",
    "    try:\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'lxml')\n",
    "        sleep(1)\n",
    "        return soup.find_all('section', {'class': 'artdeco-card pv-profile-card break-words mt2'})\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return\n",
    "\n",
    "def scrape_about(sections):\n",
    "    try:\n",
    "        about_section = next((sec for sec in sections if sec.find('div', {'id': 'about'})), None)\n",
    "        sleep(1)\n",
    "        if not about_section:\n",
    "            print(\"No About section found\")\n",
    "            profile_data['about'] = \"\"\n",
    "            return\n",
    "        try:\n",
    "            about = about_section.find('div', class_='display-flex ph5 pv3')\n",
    "            profile_data['about'] = about.get_text().strip() if about else \"\"\n",
    "            print('ABOUT DONE                  = \\n', profile_data['about'])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            profile_data['about'] = \"\"\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        profile_data['about'] = \"\"\n",
    "    \n",
    "def get_exp(exp):\n",
    "    exp_dict = {}\n",
    "\n",
    "    # Extract company name\n",
    "    try:\n",
    "        name_container = exp.find('div', {'class': 'display-flex flex-wrap align-items-center full-height'})\n",
    "        name = name_container.find('span', {'class': 'visually-hidden'}).get_text().strip() if name_container else \"\"\n",
    "    except AttributeError:\n",
    "        name = \"\"\n",
    "    exp_dict['company_name'] = name\n",
    "\n",
    "    # Extract duration\n",
    "    try:\n",
    "        duration_container = exp.find('span', {'class': 't-14 t-normal'})\n",
    "        duration = duration_container.find('span', {'class': 'visually-hidden'}).get_text().strip() if duration_container else \"\"\n",
    "    except AttributeError:\n",
    "        duration = \"\"\n",
    "    exp_dict['duration'] = duration\n",
    "\n",
    "    # get the company logo\n",
    "    try:\n",
    "        image_tag = exp.find('img', {'class': 'ivm-view-attr__img--centered EntityPhoto-square-3 evi-image lazy-image ember-view'})\n",
    "        logo = image_tag.get('src')\n",
    "    except:\n",
    "        logo = \"\"\n",
    "    exp_dict['logo'] = logo  \n",
    "\n",
    "    # Extract designations\n",
    "    designations = exp.find_all('div', {'class': 'fPLNkfiTqBJivqMiXLaRuObcmlUMZsDPkIAVk yCpXOOXwXcJnFOsCoYTsmMdAvLcplVbgNCBU'}) or []\n",
    "    item_list = []\n",
    "\n",
    "    for position in designations:\n",
    "        spans = position.find_all('span', {'class': 'visually-hidden'})\n",
    "        item_dict = {\n",
    "            'designation': spans[0].get_text().strip() if len(spans) > 0 else \"\",\n",
    "            'duration': spans[1].get_text().strip() if len(spans) > 1 else \"\",\n",
    "            'location': spans[2].get_text().strip() if len(spans) > 2 else \"\",\n",
    "            'projects': spans[3].get_text().strip() if len(spans) > 3 else \"\"\n",
    "        }\n",
    "        item_list.append(item_dict)\n",
    "\n",
    "    exp_dict['designations'] = item_list\n",
    "\n",
    "    return exp_dict\n",
    "\n",
    "\n",
    "def scrape_experience(sections):\n",
    "    try:\n",
    "        experience_section = next((sec for sec in sections if sec.find('div', {'id': 'experience'})), None)\n",
    "        sleep(1)\n",
    "        if not experience_section:\n",
    "            print(\"No Experience section found\")\n",
    "            profile_data['experience'] = []\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            experiences = experience_section.find_all('div', {'class': 'fPLNkfiTqBJivqMiXLaRuObcmlUMZsDPkIAVk SeRILEEOfWLelfcuceiLywOjAamlMoEkmnTdFk itGgYIXPpfAaqNrUDXHQVkGSUXMzldwQtdzM'})\n",
    "            profile_data['experience'] = [get_exp(exp) for exp in experiences]\n",
    "            print('EXPERIENCE DONE                   =\\n',profile_data['experience'])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            profile_data['experience'] = []\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        profile_data['experience'] = []\n",
    "\n",
    "\n",
    "def get_project(item):\n",
    "    spans = item.find_all('span', {'class': 'visually-hidden'})\n",
    "\n",
    "    item_dict = {\n",
    "        'project_name': spans[0].get_text().strip() if len(spans) > 0 else \"\",\n",
    "        'duration': spans[1].get_text().strip() if len(spans) > 1 else \"\",\n",
    "        'description': spans[2].get_text().strip() if len(spans) > 2 else \"\",\n",
    "    }\n",
    "    return item_dict\n",
    "\n",
    "\n",
    "def scrape_projects():\n",
    "    try:\n",
    "        # Wait for the 'See All Projects' button and click it\n",
    "        view_all_projects_button = wait.until(EC.element_to_be_clickable((By.ID, \"navigation-index-see-all-projects\")))\n",
    "        view_all_projects_button.click()\n",
    "        sleep(4)\n",
    "    except TimeoutException:\n",
    "        print(\"Failed to find the 'See All Projects' button.\")\n",
    "        profile_data['projects'] = []\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Wait for the projects section to load\n",
    "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'section.artdeco-card.pb3')))\n",
    "    except TimeoutException:\n",
    "        print(\"Failed to load the projects section.\")\n",
    "        profile_data['projects'] = []\n",
    "        return\n",
    "\n",
    "    # Parse the page source for project details\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'lxml')\n",
    "\n",
    "    projects_section = soup.find('section', {'class': 'artdeco-card pb3'})\n",
    "    items = projects_section.find_all('div', {'class': 'fPLNkfiTqBJivqMiXLaRuObcmlUMZsDPkIAVk SeRILEEOfWLelfcuceiLywOjAamlMoEkmnTdFk itGgYIXPpfAaqNrUDXHQVkGSUXMzldwQtdzM'}) if projects_section else []\n",
    "\n",
    "    profile_data['projects'] = [get_project(item) for item in items]\n",
    "    print('PROJECT DONE                    =\\n',profile_data['projects'])\n",
    "\n",
    "    # Navigate back to the main profile page\n",
    "    driver.back()\n",
    "    sleep(4)\n",
    "\n",
    "\n",
    "def get_skills(item):\n",
    "    spans = item.find_all('span', {'class': 'visually-hidden'})\n",
    "    return spans[0].get_text().strip() if spans else \"\"\n",
    "\n",
    "\n",
    "def scrape_skills():\n",
    "    try:\n",
    "        # Wait for the 'Show All Skills' link and click it\n",
    "        view_all_skills_link = wait.until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//a[contains(@id, 'navigation-index-Show-all-') and contains(@id, '-skills')]\"))\n",
    "        )\n",
    "        view_all_skills_link.click()\n",
    "        sleep(4)\n",
    "    except TimeoutException:\n",
    "        print(\"Failed to find or click the 'Show All Skills' link.\")\n",
    "        profile_data['skills'] = []\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Wait for the skills section to load\n",
    "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'section.artdeco-card.pb3')))\n",
    "    except TimeoutException:\n",
    "        print(\"Failed to load the skills section.\")\n",
    "        profile_data['skills'] = []\n",
    "        return\n",
    "\n",
    "    # Parse the page source for skills\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'lxml')\n",
    "\n",
    "    skills_section = soup.find('section', {'class': 'artdeco-card pb3'})\n",
    "    items = skills_section.find_all('div', {'class': 'fPLNkfiTqBJivqMiXLaRuObcmlUMZsDPkIAVk SeRILEEOfWLelfcuceiLywOjAamlMoEkmnTdFk itGgYIXPpfAaqNrUDXHQVkGSUXMzldwQtdzM'}) if skills_section else []\n",
    "\n",
    "    profile_data['skills'] = [get_skills(item) for item in items]\n",
    "    print('Skills done             =\\n',profile_data['skills'])\n",
    "\n",
    "    # Navigate back to the main profile page\n",
    "    driver.back()\n",
    "    sleep(4)\n",
    "\n",
    "\n",
    "\n",
    "def save_profile_data_to_json(profile_data, file_name=\"profile_data.json\"):\n",
    "    try:\n",
    "        # Convert Python dictionary to a JSON string and save it to a file\n",
    "        with open(file_name, 'w', encoding='utf-8') as json_file:\n",
    "            json.dump(profile_data, json_file, indent=4, ensure_ascii=False)\n",
    "        print(f\"Profile data saved to {file_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while saving JSON: {e}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # if len(sys.argv) < 2:\n",
    "    #     print(\"Usage: python scrapper.py <LinkedIn_Profile_URL>\")\n",
    "    #     sys.exit(1)\n",
    "\n",
    "    load_dotenv()\n",
    "    global driver\n",
    "    global wait\n",
    "    global profile_data\n",
    "    global url\n",
    "\n",
    "    #url = sys.argv[1]\n",
    "    url = \"https://www.linkedin.com/in/chandrasiddhartha/\"\n",
    "\n",
    "    driver = webdriver.Chrome()  # start a new window with chrome web browser\n",
    "    wait = WebDriverWait(driver, 10)  # WebDriverWait instance with a 10-second timeout\n",
    "    profile_data = {}  # dictionary to store profile data\n",
    "    sections = None # sections in linkedin (eg about section, experience section, etc)\n",
    "    try:\n",
    "        login()\n",
    "        scrape_name_headline()\n",
    "        scrape_pfp_banner()\n",
    "        sections = get_all_sections_list()\n",
    "        #print('sections = ',sections)\n",
    "        scrape_about(sections)\n",
    "        scrape_experience(sections)\n",
    "        scrape_projects()\n",
    "        scrape_skills()\n",
    "        print(\"Profile_data = \", profile_data)\n",
    "        save_profile_data_to_json(profile_data)  # saves the json file in same directory\n",
    "    except TimeoutException:\n",
    "        print(\"Timed out waiting for a page element. Ensure that the provided URL is correct or maybe a wild captcha appeared.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        #return json.dump(profile_data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: Message: invalid argument\n",
      "  (Session info: chrome=131.0.6778.109)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF6186C6CF5+28821]\n",
      "\t(No symbol) [0x00007FF618633880]\n",
      "\t(No symbol) [0x00007FF6184D55B9]\n",
      "\t(No symbol) [0x00007FF6184C3051]\n",
      "\t(No symbol) [0x00007FF6184C12FD]\n",
      "\t(No symbol) [0x00007FF6184C1B3C]\n",
      "\t(No symbol) [0x00007FF6184D885A]\n",
      "\t(No symbol) [0x00007FF6185701FE]\n",
      "\t(No symbol) [0x00007FF61854F2FA]\n",
      "\t(No symbol) [0x00007FF61856F412]\n",
      "\t(No symbol) [0x00007FF61854F0A3]\n",
      "\t(No symbol) [0x00007FF61851A778]\n",
      "\t(No symbol) [0x00007FF61851B8E1]\n",
      "\tGetHandleVerifier [0x00007FF6189FFCED+3408013]\n",
      "\tGetHandleVerifier [0x00007FF618A1745F+3504127]\n",
      "\tGetHandleVerifier [0x00007FF618A0B63D+3455453]\n",
      "\tGetHandleVerifier [0x00007FF61878BDFB+835995]\n",
      "\t(No symbol) [0x00007FF61863EB9F]\n",
      "\t(No symbol) [0x00007FF61863A854]\n",
      "\t(No symbol) [0x00007FF61863A9ED]\n",
      "\t(No symbol) [0x00007FF61862A1D9]\n",
      "\tBaseThreadInitThunk [0x00007FFBA654259D+29]\n",
      "\tRtlUserThreadStart [0x00007FFBA70CAF38+40]\n",
      "\n",
      "Profile data saved to profile_data.json\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import sys\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "\n",
    "#url = \"https://www.linkedin.com/in/laxmimerit\"  # profile to scrape\n",
    "global driver\n",
    "global wait\n",
    "global url\n",
    "global profile_data\n",
    "\n",
    "def login():\n",
    "    driver.get('https://www.linkedin.com/login')\n",
    "    try:\n",
    "        wait.until(EC.title_contains(\"LinkedIn Login\"))\n",
    "    except TimeoutException:\n",
    "        print(\"Error: Not on login page\")\n",
    "        return\n",
    "\n",
    "    email = wait.until(EC.presence_of_element_located((By.ID, 'username')))\n",
    "    email.send_keys(os.environ['EMAIL'])\n",
    "\n",
    "    password = driver.find_element(By.ID, 'password')\n",
    "    password.send_keys(os.environ['PASSWORD'])\n",
    "    password.submit()\n",
    "\n",
    "    try:\n",
    "        wait.until(EC.url_contains(\"/feed/\"))\n",
    "    except TimeoutException:\n",
    "        print(\"Error: Login failed or page not redirected\")\n",
    "\n",
    "def scrape_name_headline():\n",
    "    driver.get(url)\n",
    "    sleep(4)\n",
    "    try:\n",
    "        wait.until(EC.presence_of_element_located((By.XPATH, \"//h1[contains(@class, 'inline') and contains(@class, 't-24') and contains(@class, 'v-align-middle')]\")))\n",
    "    except TimeoutException:\n",
    "        print(\"Error: Could not load profile page or invalid URL\")\n",
    "\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'lxml')\n",
    "\n",
    "    try:\n",
    "        name_element = driver.find_element(By.XPATH, \"//h1[contains(@class, 'inline') and contains(@class, 't-24') and contains(@class, 'v-align-middle')]\")\n",
    "        name = name_element.text.strip()\n",
    "        profile_data['name'] = name\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        profile_data['name'] = \"\"\n",
    "\n",
    "    profile_data['url'] = url\n",
    "\n",
    "    try:\n",
    "        headline = soup.find('div', {'class': 'text-body-medium break-words'})\n",
    "        profile_data['headline'] = headline.get_text().strip() if headline else \"\"\n",
    "        print(\"Headline fetched \", profile_data)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        profile_data['headline'] = \"\"\n",
    "\n",
    "def scrape_pfp_banner():\n",
    "    try:\n",
    "        page_source = driver.page_source  # gets all the html code\n",
    "        soup = BeautifulSoup(page_source, 'lxml') # parse\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    try:\n",
    "        image_div = soup.find('div', {'class': 'ph5 pb5'})\n",
    "        image_tag = image_div.find('img', {'class' : 'gYwGeQHVKFihyyWibCvmHZFDQZfKneaBo pv-top-card-profile-picture__image--show evi-image ember-view'})\n",
    "        pfp_uri = image_tag.get('src')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pfp_uri=\"\"\n",
    "    profile_data['pfp'] = pfp_uri\n",
    "\n",
    "    try:\n",
    "        image_tag = soup.find('img', {'id': \"profile-background-image-target-image\"})\n",
    "        banner_uri = image_tag.get('src')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        banner_uri = \"\"\n",
    "    profile_data['banner'] = banner_uri\n",
    "    \n",
    "\n",
    "def get_all_sections_list():\n",
    "    try:\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'lxml')\n",
    "        sleep(1)\n",
    "        return soup.find_all('section', {'class': 'artdeco-card pv-profile-card break-words mt2'})\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return\n",
    "\n",
    "def scrape_about(sections):\n",
    "    try:\n",
    "        about_section = next((sec for sec in sections if sec.find('div', {'id': 'about'})), None)\n",
    "        sleep(1)\n",
    "        if not about_section:\n",
    "            print(\"No About section found\")\n",
    "            profile_data['about'] = \"\"\n",
    "            return\n",
    "        try:\n",
    "            about = about_section.find('div', class_='display-flex ph5 pv3')\n",
    "            profile_data['about'] = about.get_text().strip() if about else \"\"\n",
    "            print('ABOUT DONE                  = \\n', profile_data['about'])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            profile_data['about'] = \"\"\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        profile_data['about'] = \"\"\n",
    "    \n",
    "def get_exp(exp):\n",
    "    exp_dict = {}\n",
    "\n",
    "    # Extract company name\n",
    "    try:\n",
    "        name_container = exp.find('div', {'class': 'display-flex flex-wrap align-items-center full-height'})\n",
    "        name = name_container.find('span', {'class': 'visually-hidden'}).get_text().strip() if name_container else \"\"\n",
    "    except AttributeError:\n",
    "        name = \"\"\n",
    "    exp_dict['company_name'] = name\n",
    "\n",
    "    # Extract duration\n",
    "    try:\n",
    "        duration_container = exp.find('span', {'class': 't-14 t-normal'})\n",
    "        duration = duration_container.find('span', {'class': 'visually-hidden'}).get_text().strip() if duration_container else \"\"\n",
    "    except AttributeError:\n",
    "        duration = \"\"\n",
    "    exp_dict['duration'] = duration\n",
    "\n",
    "    # get the company logo\n",
    "    try:\n",
    "        image_tag = exp.find('img', {'class': 'ivm-view-attr__img--centered EntityPhoto-square-3 evi-image lazy-image ember-view'})\n",
    "        logo = image_tag.get('src')\n",
    "    except:\n",
    "        logo = \"\"\n",
    "    exp_dict['logo'] = logo  \n",
    "\n",
    "    # Extract designations\n",
    "    designations = exp.find_all('div', {'class': 'fPLNkfiTqBJivqMiXLaRuObcmlUMZsDPkIAVk yCpXOOXwXcJnFOsCoYTsmMdAvLcplVbgNCBU'}) or []\n",
    "    item_list = []\n",
    "\n",
    "    for position in designations:\n",
    "        spans = position.find_all('span', {'class': 'visually-hidden'})\n",
    "        item_dict = {\n",
    "            'designation': spans[0].get_text().strip() if len(spans) > 0 else \"\",\n",
    "            'duration': spans[1].get_text().strip() if len(spans) > 1 else \"\",\n",
    "            'location': spans[2].get_text().strip() if len(spans) > 2 else \"\",\n",
    "            'projects': spans[3].get_text().strip() if len(spans) > 3 else \"\"\n",
    "        }\n",
    "        item_list.append(item_dict)\n",
    "\n",
    "    exp_dict['designations'] = item_list\n",
    "\n",
    "    return exp_dict\n",
    "\n",
    "\n",
    "def scrape_experience(sections):\n",
    "    try:\n",
    "        experience_section = next((sec for sec in sections if sec.find('div', {'id': 'experience'})), None)\n",
    "        sleep(1)\n",
    "        if not experience_section:\n",
    "            print(\"No Experience section found\")\n",
    "            profile_data['experience'] = []\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            experiences = experience_section.find_all('div', {'class': 'fPLNkfiTqBJivqMiXLaRuObcmlUMZsDPkIAVk SeRILEEOfWLelfcuceiLywOjAamlMoEkmnTdFk itGgYIXPpfAaqNrUDXHQVkGSUXMzldwQtdzM'})\n",
    "            profile_data['experience'] = [get_exp(exp) for exp in experiences]\n",
    "            print('EXPERIENCE DONE                   =\\n',profile_data['experience'])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            profile_data['experience'] = []\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        profile_data['experience'] = []\n",
    "\n",
    "\n",
    "def get_project(item):\n",
    "    spans = item.find_all('span', {'class': 'visually-hidden'})\n",
    "\n",
    "    item_dict = {\n",
    "        'project_name': spans[0].get_text().strip() if len(spans) > 0 else \"\",\n",
    "        'duration': spans[1].get_text().strip() if len(spans) > 1 else \"\",\n",
    "        'description': spans[2].get_text().strip() if len(spans) > 2 else \"\",\n",
    "    }\n",
    "    return item_dict\n",
    "\n",
    "\n",
    "def scrape_projects():\n",
    "    try:\n",
    "        # Wait for the 'See All Projects' button and click it\n",
    "        view_all_projects_button = wait.until(EC.element_to_be_clickable((By.ID, \"navigation-index-see-all-projects\")))\n",
    "        view_all_projects_button.click()\n",
    "        sleep(4)\n",
    "    except TimeoutException:\n",
    "        print(\"Failed to find the 'See All Projects' button.\")\n",
    "        profile_data['projects'] = []\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Wait for the projects section to load\n",
    "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'section.artdeco-card.pb3')))\n",
    "    except TimeoutException:\n",
    "        print(\"Failed to load the projects section.\")\n",
    "        profile_data['projects'] = []\n",
    "        return\n",
    "\n",
    "    # Parse the page source for project details\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'lxml')\n",
    "\n",
    "    projects_section = soup.find('section', {'class': 'artdeco-card pb3'})\n",
    "    items = projects_section.find_all('div', {'class': 'fPLNkfiTqBJivqMiXLaRuObcmlUMZsDPkIAVk SeRILEEOfWLelfcuceiLywOjAamlMoEkmnTdFk itGgYIXPpfAaqNrUDXHQVkGSUXMzldwQtdzM'}) if projects_section else []\n",
    "\n",
    "    profile_data['projects'] = [get_project(item) for item in items]\n",
    "    print('PROJECT DONE                    =\\n',profile_data['projects'])\n",
    "\n",
    "    # Navigate back to the main profile page\n",
    "    driver.back()\n",
    "    sleep(4)\n",
    "\n",
    "\n",
    "def get_skills(item):\n",
    "    spans = item.find_all('span', {'class': 'visually-hidden'})\n",
    "    return spans[0].get_text().strip() if spans else \"\"\n",
    "\n",
    "\n",
    "def scrape_skills():\n",
    "    try:\n",
    "        # Wait for the 'Show All Skills' link and click it\n",
    "        view_all_skills_link = wait.until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//a[contains(@id, 'navigation-index-Show-all-') and contains(@id, '-skills')]\"))\n",
    "        )\n",
    "        view_all_skills_link.click()\n",
    "        sleep(4)\n",
    "    except TimeoutException:\n",
    "        print(\"Failed to find or click the 'Show All Skills' link.\")\n",
    "        profile_data['skills'] = []\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Wait for the skills section to load\n",
    "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'section.artdeco-card.pb3')))\n",
    "    except TimeoutException:\n",
    "        print(\"Failed to load the skills section.\")\n",
    "        profile_data['skills'] = []\n",
    "        return\n",
    "\n",
    "    # Parse the page source for skills\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'lxml')\n",
    "\n",
    "    skills_section = soup.find('section', {'class': 'artdeco-card pb3'})\n",
    "    items = skills_section.find_all('div', {'class': 'fPLNkfiTqBJivqMiXLaRuObcmlUMZsDPkIAVk SeRILEEOfWLelfcuceiLywOjAamlMoEkmnTdFk itGgYIXPpfAaqNrUDXHQVkGSUXMzldwQtdzM'}) if skills_section else []\n",
    "\n",
    "    profile_data['skills'] = [get_skills(item) for item in items]\n",
    "    print('Skills done             =\\n',profile_data['skills'])\n",
    "\n",
    "    # Navigate back to the main profile page\n",
    "    driver.back()\n",
    "    sleep(4)\n",
    "\n",
    "\n",
    "\n",
    "def save_profile_data_to_json(profile_data, file_name=\"profile_data.json\"):\n",
    "    try:\n",
    "        # Convert Python dictionary to a JSON string and save it to a file\n",
    "        with open(file_name, 'w', encoding='utf-8') as json_file:\n",
    "            json.dump(profile_data, json_file, indent=4, ensure_ascii=False)\n",
    "        print(f\"Profile data saved to {file_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while saving JSON: {e}\")\n",
    "\n",
    "def scrape_linkedin_profile(profile_url):\n",
    "    load_dotenv()\n",
    "    global driver\n",
    "    global wait\n",
    "    global profile_data\n",
    "    global url\n",
    "\n",
    "    url = profile_url\n",
    "\n",
    "    driver = webdriver.Chrome()  # start a new window with chrome web browser\n",
    "    wait = WebDriverWait(driver, 10)  # WebDriverWait instance with a 10-second timeout\n",
    "    profile_data = {}  # dictionary to store profile data\n",
    "    sections = None # sections in linkedin (eg about section, experience section, etc)\n",
    "    try:\n",
    "        login()\n",
    "        scrape_name_headline()\n",
    "        scrape_pfp_banner()\n",
    "        sections = get_all_sections_list()\n",
    "        #print('sections = ',sections)\n",
    "        scrape_about(sections)\n",
    "        scrape_experience(sections)\n",
    "        scrape_projects()\n",
    "        scrape_skills()\n",
    "        print(\"Profile_data = \", profile_data)\n",
    "    except TimeoutException:\n",
    "        print(\"Timed out waiting for a page element. Ensure that the provided URL is correct or maybe a wild captcha appeared.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        return save_profile_data_to_json(profile_data)  # Saves the JSON file in the same directory\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) < 2:\n",
    "        print(\"Usage: python scraper.py <LinkedIn_Profile_URL>\")\n",
    "        sys.exit(1)\n",
    "    profile_url = sys.argv[1]\n",
    "    scrape_linkedin_profile(profile_url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
